#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SenseVoice ä¼˜åŒ–ç‰ˆè½¬å½•è„šæœ¬ - é’ˆå¯¹å¤šGPUé«˜æ€§èƒ½è½¬å½•
æ”¯æŒGPUå†…å­˜ç®¡ç†ã€æ‰¹å¤„ç†ä¼˜åŒ–ã€å¹¶è¡Œå¤„ç†
"""

import sys
import json
import os
import argparse
import time
import gc
from pathlib import Path
import torch
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess
from modelscope import snapshot_download

# è®¾ç½®ç¼“å­˜ç›®å½•
cache_dir = os.path.expanduser("~/.cache/funasr")
os.makedirs(cache_dir, exist_ok=True)

def get_optimal_gpu():
    """è·å–æœ€ä¼˜GPUè®¾å¤‡"""
    if not torch.cuda.is_available():
        return "cpu"

    gpu_count = torch.cuda.device_count()
    if gpu_count == 0:
        return "cpu"

    # é€‰æ‹©æ˜¾å­˜æœ€å¤§ä¸”åˆ©ç”¨ç‡æœ€ä½çš„GPU
    best_gpu = 0
    best_score = -1

    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        total_memory = props.total_memory

        # æ£€æŸ¥å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        torch.cuda.set_device(i)
        allocated = torch.cuda.memory_allocated(i)
        reserved = torch.cuda.memory_reserved(i)
        free_memory = total_memory - reserved

        # è®¡ç®—åˆ†æ•°ï¼šä¼˜å…ˆé€‰æ‹©æ˜¾å­˜å¤§ä¸”ç©ºé—²çš„GPU
        score = free_memory / (1024**3)  # GB

        print(f"ğŸ® GPU {i}: {props.name}, æ€»æ˜¾å­˜: {total_memory/(1024**3):.1f}GB, ç©ºé—²: {free_memory/(1024**3):.1f}GB", file=sys.stderr)

        if score > best_score:
            best_score = score
            best_gpu = i

    return f"cuda:{best_gpu}"

def clear_gpu_cache():
    """æ¸…ç†GPUç¼“å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

def optimize_model_settings(device):
    """æ ¹æ®è®¾å¤‡ä¼˜åŒ–æ¨¡å‹è®¾ç½®"""
    settings = {
        "batch_size_s": 500,
        "merge_length_s": 30,
        "max_single_segment_time": 30000,
        "max_end_silence_time": 500,
    }

    if device.startswith("cuda"):
        # GPUä¼˜åŒ–è®¾ç½®
        gpu_num = int(device.split(":")[1])
        props = torch.cuda.get_device_properties(gpu_num)
        memory_gb = props.total_memory / (1024**3)

        if memory_gb >= 12:  # 12GB+ GPU
            settings.update({
                "batch_size_s": 1000,  # å¢å¤§æ‰¹å¤„ç†
                "merge_length_s": 60,   # å¢å¤§åˆå¹¶é•¿åº¦
            })
        elif memory_gb >= 8:   # 8GB+ GPU
            settings.update({
                "batch_size_s": 800,
                "merge_length_s": 45,
            })
    else:
        # CPUä¼˜åŒ–è®¾ç½®
        settings.update({
            "batch_size_s": 100,
            "merge_length_s": 15,
        })

    return settings

def transcribe_audio_optimized(audio_path, language="auto", use_itn=True):
    """
    ä¼˜åŒ–ç‰ˆéŸ³é¢‘è½¬å½•
    """
    start_time = time.time()

    print(f"ğŸš€ SenseVoiceä¼˜åŒ–è½¬å½•: {os.path.basename(audio_path)}", file=sys.stderr)

    try:
        # æ¸…ç†GPUç¼“å­˜
        clear_gpu_cache()

        # è·å–æœ€ä¼˜è®¾å¤‡
        device = get_optimal_gpu()
        print(f"ğŸ¯ é€‰å®šè®¾å¤‡: {device}", file=sys.stderr)

        # ä¼˜åŒ–è®¾ç½®
        settings = optimize_model_settings(device)
        print(f"âš™ï¸ ä¼˜åŒ–å‚æ•°: batch_size={settings['batch_size_s']}, merge_length={settings['merge_length_s']}", file=sys.stderr)

        # ä¸‹è½½æ¨¡å‹ï¼ˆé‡å®šå‘è¾“å‡ºåˆ°stderrï¼‰
        import contextlib
        from io import StringIO

        # æ•è·æ¨¡å‹ä¸‹è½½çš„stdoutè¾“å‡º
        f = StringIO()
        with contextlib.redirect_stdout(f):
            model_dir = snapshot_download("iic/SenseVoiceSmall", cache_dir=cache_dir)

        # å°†ä¸‹è½½ä¿¡æ¯è¾“å‡ºåˆ°stderr
        download_info = f.getvalue()
        if download_info.strip():
            print(f"ğŸ“¦ æ¨¡å‹ä¸‹è½½ä¿¡æ¯: {download_info.strip()}", file=sys.stderr)

        # åˆå§‹åŒ–æ¨¡å‹
        print(f"ğŸ”„ åŠ è½½SenseVoiceæ¨¡å‹åˆ°{device}...", file=sys.stderr)

        # è®¾ç½®PyTorchä¼˜åŒ–
        if device.startswith("cuda"):
            torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–CUDNNæ€§èƒ½
            torch.backends.cudnn.deterministic = False

        # æ•è·æ¨¡å‹åˆå§‹åŒ–çš„è¾“å‡º
        f2 = StringIO()
        with contextlib.redirect_stdout(f2):
            model = AutoModel(
                model=model_dir,
                trust_remote_code=True,
                remote_code="./model.py",
                vad_model="fsmn-vad",
                vad_kwargs={
                    "max_single_segment_time": settings["max_single_segment_time"],
                    "max_end_silence_time": settings["max_end_silence_time"],
                },
                device=device,
                # æ·»åŠ æ€§èƒ½ä¼˜åŒ–å‚æ•°
                ncpu=4 if device == "cpu" else 1,  # CPUçº¿ç¨‹æ•°
            )

        # å°†æ¨¡å‹åˆå§‹åŒ–ä¿¡æ¯è¾“å‡ºåˆ°stderr
        init_info = f2.getvalue()
        if init_info.strip():
            print(f"ğŸ—ï¸ æ¨¡å‹åˆå§‹åŒ–ä¿¡æ¯: {init_info.strip()}", file=sys.stderr)

        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹è½¬å½•...", file=sys.stderr)

        # æ‰§è¡Œè½¬å½•
        res = model.generate(
            input=audio_path,
            cache={},
            language=language,
            use_itn=use_itn,
            batch_size_s=settings["batch_size_s"],
            merge_vad=True,
            merge_length_s=settings["merge_length_s"],
            pred_timestamp=True,
            # æ€§èƒ½ä¼˜åŒ–å‚æ•°
            disable_pbar=False,  # æ˜¾ç¤ºè¿›åº¦æ¡
        )

        # å¤„ç†ç»“æœ
        if not res or len(res) == 0:
            raise ValueError("è½¬å½•ç»“æœä¸ºç©º")

        full_text = res[0]["text"] if isinstance(res[0], dict) else res[0].get("text", "")

        # åå¤„ç†
        if use_itn:
            full_text = rich_transcription_postprocess(full_text)

        # æå–ä¿¡æ¯
        detected_language = res[0].get("language", language if language != "auto" else "zh")
        emotion = res[0].get("emotion", None)
        events = res[0].get("event", [])

        # æ„å»ºç‰‡æ®µ
        segments = []
        if "segments" in res[0]:
            for seg in res[0]["segments"]:
                segment = {
                    "start": seg.get("start", 0),
                    "end": seg.get("end", 0),
                    "text": seg.get("text", ""),
                }
                if "speaker" in seg:
                    segment["speaker"] = seg["speaker"]
                segments.append(segment)
        else:
            segments = [{
                "start": 0,
                "end": len(full_text) * 0.15,  # ä¼˜åŒ–ä¼°ç®—
                "text": full_text
            }]

        elapsed_time = time.time() - start_time

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        audio_duration = segments[-1]["end"] if segments else 0
        rtf = elapsed_time / max(audio_duration, 1)  # å®æ—¶å› å­

        print(f"ğŸ‰ è½¬å½•å®Œæˆ!", file=sys.stderr)
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡: {len(full_text)}å­—ç¬¦, {elapsed_time:.1f}ç§’, RTF={rtf:.3f}", file=sys.stderr)

        # æ¸…ç†å†…å­˜
        del model
        clear_gpu_cache()

        result = {
            "success": True,
            "text": full_text,
            "segments": segments,
            "language": detected_language,
            "duration": elapsed_time,
            "model": "SenseVoiceSmall-Optimized",
            "stats": {
                "total_characters": len(full_text),
                "total_segments": len(segments),
                "processing_time": elapsed_time,
                "rtf": rtf,
                "audio_duration": audio_duration,
                "device": device,
                "batch_size": settings["batch_size_s"],
                "audio_file": os.path.basename(audio_path)
            }
        }

        if emotion:
            result["emotion"] = emotion
        if events:
            result["events"] = events

        return result

    except Exception as e:
        clear_gpu_cache()
        print(f"âŒ è½¬å½•å¤±è´¥: {str(e)}", file=sys.stderr)
        return {
            "success": False,
            "error": str(e),
            "text": "",
            "segments": []
        }

def main():
    parser = argparse.ArgumentParser(description='SenseVoice ä¼˜åŒ–ç‰ˆè½¬å½•å·¥å…·')
    parser.add_argument('audio_file', help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--language', default='auto',
                      choices=['auto', 'zh', 'en', 'yue', 'ja', 'ko'],
                      help='è¯­è¨€è®¾ç½® (é»˜è®¤: auto)')
    parser.add_argument('--no-itn', action='store_true',
                      help='ç¦ç”¨æ•°å­—è§„èŒƒåŒ–ï¼ˆITNï¼‰')
    parser.add_argument('--save-transcript', help='ä¿å­˜è½¬å½•æ–‡æœ¬çš„ç›®å½•')
    parser.add_argument('--file-prefix', default='sensevoice-opt',
                      help='ä¿å­˜æ–‡ä»¶çš„å‰ç¼€')
    parser.add_argument('--podcast-title', default='Untitled',
                      help='æ’­å®¢æ ‡é¢˜')
    parser.add_argument('--source-url', default='',
                      help='æºURLï¼ˆå¯é€‰ï¼‰')

    args = parser.parse_args()

    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶
    if not os.path.exists(args.audio_file):
        print(json.dumps({
            "success": False,
            "error": f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {args.audio_file}",
            "text": "",
            "segments": []
        }, ensure_ascii=False))
        sys.exit(1)

    # æ‰§è¡Œè½¬å½•
    result = transcribe_audio_optimized(
        args.audio_file,
        language=args.language,
        use_itn=not args.no_itn
    )

    # ä¿å­˜æ–‡ä»¶ï¼ˆå¤ç”¨åŸè„šæœ¬çš„ä¿å­˜å‡½æ•°ï¼‰
    if args.save_transcript and result["success"]:
        from sensevoice_transcribe import save_transcript_files
        saved_files = save_transcript_files(
            result,
            args.save_transcript,
            args.file_prefix,
            args.podcast_title
        )
        result["savedFiles"] = saved_files

    # è¾“å‡ºç»“æœ
    print(json.dumps(result, ensure_ascii=False, indent=2))
    sys.exit(0 if result["success"] else 1)

if __name__ == "__main__":
    main()