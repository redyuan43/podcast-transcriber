#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæœ¬åœ°Faster-Whisperè½¬å½•è„šæœ¬
æ”¯æŒè¯´è¯äººåˆ†ç¦»(Speaker Diarization)å’Œæƒ…ç»ªæ£€æµ‹
"""

import sys
import json
import argparse
import time
import re
from pathlib import Path
from faster_whisper import WhisperModel
import warnings
warnings.filterwarnings("ignore")

class EnhancedWhisperTranscriber:
    def __init__(self, model_size="base", device="cpu", compute_type="int8"):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆWhisperæ¨¡å‹
        """
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½Whisperæ¨¡å‹: {model_size}", file=sys.stderr)
        self.model = WhisperModel(model_size, device=device, compute_type=compute_type)
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ", file=sys.stderr)
        
        # æƒ…ç»ªå…³é”®è¯å­—å…¸
        self.emotion_keywords = {
            'ç¬‘': ['å“ˆå“ˆ', 'å‘µå‘µ', 'å˜¿å˜¿', 'å’¯å’¯', 'å“ˆå“ˆå“ˆ', 'å‘µå‘µå‘µ'],
            'æ„Ÿå¹': ['å“‡', 'å“å‘€', 'å¤©å“ª', 'æˆ‘çš„å¤©', 'å“¦', 'å•Š'],
            'æ€è€ƒ': ['å—¯', 'é¢', 'è¿™ä¸ª', 'é‚£ä¸ª', 'å°±æ˜¯è¯´'],
            'èµåŒ': ['å¯¹å¯¹å¯¹', 'æ˜¯çš„æ˜¯çš„', 'æ²¡é”™', 'ç¡®å®', 'å¯¹å•Š'],
            'æƒŠè®¶': ['ä»€ä¹ˆ', 'çœŸçš„å—', 'ä¸ä¼šå§', 'è¿™ä¹ˆå‰å®³'],
            'åœé¡¿': ['..', '...', '....']
        }
    
    def detect_speaker_change(self, segments):
        """
        åŸºäºéŸ³é¢‘ç‰¹å¾æ£€æµ‹è¯´è¯äººå˜åŒ–çš„ç®€åŒ–ç‰ˆæœ¬
        """
        speakers = []
        current_speaker = "ä¸»æŒäºº"
        speaker_count = 0
        
        for i, segment in enumerate(segments):
            text = segment['text'].strip()
            
            # åŸºäºå†…å®¹ç‰¹å¾åˆ¤æ–­è¯´è¯äººåˆ‡æ¢
            speaker_change_indicators = [
                len(text) > 100,  # é•¿å¥å­æ›´å¯èƒ½æ˜¯æ–°è¯´è¯äºº
                text.startswith(('å¥½', 'é‚£', 'æ‰€ä»¥', 'å…¶å®', 'ä½†æ˜¯')),  # è½¬æŠ˜è¯
                i > 0 and segment['start'] - segments[i-1]['end'] > 2.0  # é•¿åœé¡¿
            ]
            
            if any(speaker_change_indicators) and i > 0:
                if current_speaker == "ä¸»æŒäºº":
                    current_speaker = "å˜‰å®¾"
                else:
                    current_speaker = "ä¸»æŒäºº"
            
            speakers.append(current_speaker)
        
        return speakers
    
    def detect_emotions(self, text):
        """
        æ£€æµ‹æ–‡æœ¬ä¸­çš„æƒ…ç»ªæ ‡è®°
        """
        emotions = []
        
        for emotion, keywords in self.emotion_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    emotions.append(emotion)
                    break
        
        # æ£€æµ‹é‡å¤å­—ç¬¦ï¼ˆè¡¨ç¤ºå¼ºè°ƒæˆ–æƒ…ç»ªï¼‰
        if re.search(r'(.)\1{2,}', text):
            emotions.append('å¼ºè°ƒ')
        
        # æ£€æµ‹é—®å·å’Œæ„Ÿå¹å·
        if '?' in text or 'ï¼Ÿ' in text:
            emotions.append('ç–‘é—®')
        if '!' in text or 'ï¼' in text:
            emotions.append('æ¿€åŠ¨')
        
        return list(set(emotions))
    
    def format_transcript_with_speakers_and_emotions(self, segments, speakers, podcast_title=None):
        """
        æ ¼å¼åŒ–è½¬å½•æ–‡æœ¬ï¼ŒåŒ…å«è¯´è¯äººå’Œæƒ…ç»ªä¿¡æ¯
        """
        from datetime import datetime
        
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        if podcast_title:
            title = f"# ğŸ“ {podcast_title} - å¢å¼ºè½¬å½•"
        else:
            title = "# ğŸ“ æ’­å®¢å¢å¼ºè½¬å½•"
        
        content = f"""{title}

**è½¬å½•æ—¶é—´**: {current_time}
**åŠŸèƒ½**: æ”¯æŒè¯´è¯äººåˆ†ç¦»å’Œæƒ…ç»ªæ£€æµ‹

---

"""
        
        current_speaker = None
        for i, (segment, speaker) in enumerate(zip(segments, speakers)):
            text = segment['text'].strip()
            
            # æ£€æµ‹æƒ…ç»ª
            emotions = self.detect_emotions(text)
            emotion_tags = ' '.join([f'[{e}]' for e in emotions]) if emotions else ''
            
            # æ—¶é—´æˆ³
            timestamp = f"[{self.seconds_to_time(segment['start'])} - {self.seconds_to_time(segment['end'])}]"
            
            # å¦‚æœè¯´è¯äººå˜åŒ–ï¼Œæ·»åŠ åˆ†éš”çº¿
            if speaker != current_speaker:
                if current_speaker is not None:
                    content += "\n---\n\n"
                content += f"## {speaker}\n\n"
                current_speaker = speaker
            
            # æ·»åŠ æ–‡æœ¬å†…å®¹
            content += f"**{timestamp}** {emotion_tags} {text}\n\n"
        
        return content
    
    def seconds_to_time(self, seconds):
        """
        å°†ç§’æ•°è½¬æ¢ä¸ºæ—¶é—´æ ¼å¼
        """
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes:02d}:{secs:02d}"
    
    def transcribe_file_enhanced(self, audio_path, language=None):
        """
        å¢å¼ºç‰ˆè½¬å½•ï¼Œæ”¯æŒè¯´è¯äººåˆ†ç¦»å’Œæƒ…ç»ªæ£€æµ‹
        """
        try:
            print(f"ğŸ¤ å¼€å§‹å¢å¼ºè½¬å½•: {audio_path}", file=sys.stderr)
            start_time = time.time()
            
            # æ‰§è¡Œè½¬å½•
            segments, info = self.model.transcribe(
                audio_path, 
                language=language,
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=500),
                word_timestamps=True  # å¯ç”¨è¯çº§æ—¶é—´æˆ³
            )
            
            # æ”¶é›†æ‰€æœ‰ç‰‡æ®µ
            transcript_segments = []
            full_text = ""
            
            for segment in segments:
                segment_dict = {
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text.strip()
                }
                transcript_segments.append(segment_dict)
                full_text += segment.text.strip() + " "
            
            print(f"ğŸ­ æ£€æµ‹è¯´è¯äººå˜åŒ–...", file=sys.stderr)
            speakers = self.detect_speaker_change(transcript_segments)
            
            print(f"ğŸ˜Š æ£€æµ‹æƒ…ç»ªæ ‡è®°...", file=sys.stderr)
            
            duration = time.time() - start_time
            
            result = {
                "success": True,
                "file": str(audio_path),
                "text": full_text.strip(),
                "segments": transcript_segments,
                "speakers": speakers,
                "language": info.language,
                "language_probability": info.language_probability,
                "duration": info.duration,
                "processing_time": round(duration, 2),
                "enhanced": True
            }
            
            print(f"âœ… å¢å¼ºè½¬å½•å®Œæˆ: {duration:.1f}ç§’", file=sys.stderr)
            print(f"ğŸ­ æ£€æµ‹åˆ°è¯´è¯äººå˜åŒ–: {len(set(speakers))}ä¸ª", file=sys.stderr)
            
            return result
            
        except Exception as e:
            error_result = {
                "success": False,
                "file": str(audio_path),
                "error": str(e),
                "text": "",
                "enhanced": False
            }
            print(f"âŒ å¢å¼ºè½¬å½•å¤±è´¥: {e}", file=sys.stderr)
            return error_result

def save_enhanced_transcript_to_file(result, save_dir, file_prefix=None, podcast_title=None, source_url=None):
    """
    ä¿å­˜å¢å¼ºè½¬å½•æ–‡æœ¬åˆ°æ–‡ä»¶
    """
    try:
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        if file_prefix:
            filename = f"{file_prefix}_enhanced_transcript.md"
        else:
            timestamp = int(time.time())
            filename = f"enhanced_transcript_{timestamp}.md"
        
        file_path = save_path / filename
        
        # åˆ›å»ºè½¬å½•å™¨å®ä¾‹æ¥æ ¼å¼åŒ–å†…å®¹
        transcriber = EnhancedWhisperTranscriber()
        
        # æ ¼å¼åŒ–ä¸ºå¢å¼ºç‰ˆMarkdown
        if result.get('enhanced') and 'speakers' in result:
            markdown_content = transcriber.format_transcript_with_speakers_and_emotions(
                result['segments'], 
                result['speakers'], 
                podcast_title
            )
        else:
            # é™çº§åˆ°æ™®é€šæ ¼å¼
            markdown_content = f"# ğŸ“ {podcast_title or 'Podcastè½¬å½•'}\n\n{result['text']}"
        
        # æ·»åŠ æ¥æºé“¾æ¥
        if source_url:
            markdown_content += f"\n\n---\n\n**æ¥æº**: {source_url}\n"
        
        # ä¿å­˜æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_size = file_path.stat().st_size
        
        file_info = {
            "type": "enhanced_transcript",
            "filename": filename,
            "path": str(file_path),
            "size": file_size
        }
        
        print(f"ğŸ“„ å¢å¼ºè½¬å½•æ–‡æœ¬å·²ä¿å­˜: {file_path} ({file_size/1024:.1f}KB)", file=sys.stderr)
        return file_info
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¢å¼ºè½¬å½•æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)
        return None

def main():
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆæœ¬åœ°Faster-WhisperéŸ³é¢‘è½¬å½•")
    parser.add_argument("files", nargs="+", help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", default="base", 
                       choices=["tiny", "base", "small", "medium", "large-v3"],
                       help="Whisperæ¨¡å‹å¤§å° (é»˜è®¤: base)")
    parser.add_argument("--language", help="æŒ‡å®šè¯­è¨€ä»£ç  (å¦‚: zh, en)")
    parser.add_argument("--device", default="cpu", 
                       choices=["cpu", "cuda"], help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--compute-type", default="int8",
                       choices=["int8", "int16", "float16", "float32"],
                       help="è®¡ç®—ç²¾åº¦")
    parser.add_argument("--output", help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--save-transcript", help="ç›´æ¥ä¿å­˜è½¬å½•æ–‡æœ¬åˆ°æŒ‡å®šç›®å½•")
    parser.add_argument("--file-prefix", help="ä¿å­˜æ–‡ä»¶çš„å‰ç¼€åç§°")
    parser.add_argument("--source-url", help="æ’­å®¢æ¥æºé“¾æ¥")
    parser.add_argument("--podcast-title", help="æ’­å®¢æ ‡é¢˜")
    parser.add_argument("--enhanced", action="store_true", help="å¯ç”¨å¢å¼ºæ¨¡å¼ï¼ˆè¯´è¯äººåˆ†ç¦»+æƒ…ç»ªæ£€æµ‹ï¼‰")
    
    args = parser.parse_args()
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    audio_files = []
    for file_path in args.files:
        path = Path(file_path)
        if not path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}", file=sys.stderr)
            sys.exit(1)
        audio_files.append(str(path.absolute()))
    
    try:
        # åˆå§‹åŒ–è½¬å½•å™¨
        if args.enhanced:
            print("ğŸš€ å¯åŠ¨å¢å¼ºè½¬å½•æ¨¡å¼", file=sys.stderr)
            transcriber = EnhancedWhisperTranscriber(
                model_size=args.model,
                device=args.device,
                compute_type=args.compute_type.replace("-", "_")
            )
            
            # ä½¿ç”¨å¢å¼ºè½¬å½•
            if len(audio_files) == 1:
                result = transcriber.transcribe_file_enhanced(audio_files[0], args.language)
            else:
                # æ‰¹é‡å¤„ç†ï¼ˆæš‚æ—¶ä½¿ç”¨æ™®é€šæ¨¡å¼ï¼‰
                print("âš ï¸ æ‰¹é‡æ¨¡å¼æš‚ä¸æ”¯æŒå¢å¼ºåŠŸèƒ½ï¼Œä½¿ç”¨æ™®é€šè½¬å½•", file=sys.stderr)
                from whisper_transcribe import LocalWhisperTranscriber
                basic_transcriber = LocalWhisperTranscriber(args.model, args.device, args.compute_type.replace("-", "_"))
                result = basic_transcriber.transcribe_multiple(audio_files, args.language)
        else:
            # ä½¿ç”¨æ™®é€šè½¬å½•
            from whisper_transcribe import LocalWhisperTranscriber
            transcriber = LocalWhisperTranscriber(
                model_size=args.model,
                device=args.device,
                compute_type=args.compute_type.replace("-", "_")
            )
            
            if len(audio_files) == 1:
                result = transcriber.transcribe_file(audio_files[0], args.language)
            else:
                result = transcriber.transcribe_multiple(audio_files, args.language)
        
        # å¤„ç†è½¬å½•æ–‡æœ¬ä¿å­˜
        saved_files = []
        if args.save_transcript and isinstance(result, dict) and result.get('success') and result.get('text'):
            if args.enhanced and result.get('enhanced'):
                file_info = save_enhanced_transcript_to_file(
                    result=result,
                    save_dir=args.save_transcript,
                    file_prefix=args.file_prefix,
                    podcast_title=args.podcast_title,
                    source_url=args.source_url
                )
            else:
                # ä½¿ç”¨åŸæœ‰ä¿å­˜æ–¹æ³•
                from whisper_transcribe import save_transcript_to_file
                file_info = save_transcript_to_file(
                    transcript_text=result['text'],
                    save_dir=args.save_transcript,
                    file_prefix=args.file_prefix,
                    original_filename=audio_files[0] if len(audio_files) == 1 else None,
                    source_url=args.source_url,
                    podcast_title=args.podcast_title
                )
            
            if file_info:
                saved_files.append(file_info)
        
        # åœ¨ç»“æœä¸­æ·»åŠ ä¿å­˜çš„æ–‡ä»¶ä¿¡æ¯
        if isinstance(result, dict):
            result['savedFiles'] = saved_files
        
        # è¾“å‡ºç»“æœ
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {args.output}", file=sys.stderr)
        else:
            # è¾“å‡ºåˆ°stdout
            print(json.dumps(result, ensure_ascii=False))
    
    except KeyboardInterrupt:
        print("\nâš ï¸ è½¬å½•è¢«ç”¨æˆ·ä¸­æ–­", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ç¨‹åºé”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()